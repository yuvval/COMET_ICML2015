%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% COMET ICML 2015 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

%% COMET packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

%%%%%%%%%%%%%%%%%%%%%%%%


% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For theorems and proofs
\usepackage{amsthm}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Metric learning using Block Coordinate Descent}

\begin{document} 

\twocolumn[
\icmltitle{Metric Learning using Block Coordinate Descent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Yuval Atzmon}{yuval.atzmon@biu.ac.il}
\icmladdress{The Gonda Brain Research Center, Bar Ilan University, 52900, Israel}
\icmlauthor{Uri Shalit}{ \todo{Uris email}}
\icmladdress{\todo{uri affiliation}}
\icmlauthor{Gal Chechik}{ \todo{Gals email}}
\icmladdress{The Gonda Brain Research Center, Bar Ilan University, 52900, Israel}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{metric learning, similarity learning}

\vskip 0.3in
]

%%%% COMET commands %%%%
\newcommand\todo[1]{\textbf{<ToDo:#1}!}
%\newcommand\mat[1]{\mathcal{#1}}
%\newcommand\mat[1]{\boldmath{#1}}
\newcommand\mat[1]{{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\newcommand{\T}{{}^\mathsf{T}}
\newcommand{\W}{\mat{W}}
\newcommand{\E}{\mat{E}}
\newcommand{\newW}{{\mat{W^*}}}
\newcommand{\eqdef}{\doteq}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\tL}{\tilde{L}(\W)}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}


%\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 
TBD
\end{abstract} 

\section{Introduction}
Learning a measure of pair-wise similarity or distance among data samples is a fundamental task in machine learning. Learned metrics can be used to retrieve a similar image or document given an example \cite{}, or be used as a representation for a supervised learning technique that is based on distances, like nearest-neighbors or kernel methods \cite{}.


\section{Related work}
There has been a lot of work on learning distance metrics and similarity measures from data, see \cite{bellet2013survey, kulis2012survey, yang2006survey} for surveys. Specifically, several approaches have been proposed for learning a positive definite (PD) Mahalanobis matrix. Learning a PD matrix is a difficult task because it requires maintaining all eigen-values positive (or at least non-negative) during the optimization process. 

One family of approached views learning Mahalanobis distances, as learning a linear projection of the data into another space (often of lower dimensionality), where a Euclidean distance is
defined among pairs of objects. Such approaches include Fisherâ€™s Linear Discriminant Analysis
(LDA), relevant component analysis (RCA) (\todo{Bar-Hillel et al., 2003)}, supervised global
metric learning \todo{(Xing et al., 2003)}, large margin nearest neighbor (LMNN) \todo{(Weinberger
et al., 2006)} and Metric Learning by Collapsing Classes \todo{(Globerson and Roweis, 2006)}.

Another family of approaches use an information measure for the optimization problem. Most notably are Information-Theoretic Metric Learning \todo{
Davis et al. (2007)} which introduced LogDet divergence regularization and LEGO \cite{lego} based on work by \todo{Davis et al. (2007)}, which learns a linear model of a [dis-]similarity function between documents in an online way.

A third family of approaches \cite{boost} \todo{cite (Bi et al. (2011), Liu and Vemuri (2012) } learn the metric matrix using rank-1 (PSD) updates which are generated by solving a dual optimization problem within a boosting-based learning process \todo{cite AdaBoost}.

\todo{Talk about bilinear similarity learning (OASIS/POLA/..)}

Recently, new approaches have been suggested to learn metrics at regimes of high dimensional sparse data. Notably \cite{hdsl} is comprised on rank-1 sparse update matrices that are all zeros except for $2\times2$ pair of features elements. It greedily incorporates one pair of features at a time to control the number of active features. \cite{qian2014} introduces randomized low rank matrix approximation and adaptive sampling of the constraints which is was developed on \cite{qian}. \cite{qian} however, still requires a costly projection step onto the PSD cone after every gradient step which scales in $o(d^3)$ due to the eigenvalue decomposition.

\todo{Talk about schur complement and row-by-row work from 2009 ,\cite(rbr))}

\section{Coordinate-descent metric learning}
We learn the metric using pairwise relations. One approach is to measure the similarity of two samples $\vec{q}, \vec{p} \in \Rd$ using a bilinear form parametrized by a model $\W \in \mathbb{R}^{d \times d}$.
\begin{equation}
S_{\W}(\vec{q}, \vec{p}) = \vec{q}\T \W \vec{p}
\end{equation}
To train our model, we generate a batch of triplets, such that three instances are sampled per triplet ($t$): a query $\vec{q}_t \in \Rd$ and two samples $\vec{p}_{t}^{+}, \vec{p}_{t}^{-} \in \Rd$. We wish that the model assigns a higher similarity score to the pair $(\vec{q}_t, \vec{p}_{t}^{+})$ than the pair $(\vec{q}_t, \vec{p}_{t}^{-})$, and hence use the ranking loss per triplet $t$. We could choose any of linear hinge loss, quadratic hinge loss or logarithmic loss. We've chosen the linear hinge loss which is easy to implement and has demonstrated good results.  loss is defined as:
\begin{equation}
l_{\W}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})=[1-\vec{q}_{t}\T \W \vec{p}_{t}^{+}+\vec{q}_{t}\T\W\vec{p}_{t}^{-}]_{+}
\label{hingelt}
\end{equation}
where $[z]_{+} \eqdef max(0,z)$. Such models have shown to achieve high precision \cite{oasis, qian}.

An outline of our next steps is that we will define an objective function of the total hinge loss over all the triplets, a matrix Frobenius norm regularization and a $\log \det$ regularization over $\W$ that assigns a high cost when $\W$ reaches close to the PD cone boundaries. 
We will take a block coordinate descent approach which ensures that each step is taken within the PD cone. Each coordinate step is taken over a single row and column $k$ of $\W$. An upper limit on the step size is  evaluated using the Schur Complement condition for positive definiteness \todo{Add citation} and we take a step with a step size within that boundary. 

\subsection{The objective}
We define an objective function
\begin{equation}
L(\W)=\sum\limits_{t\in T}{l_{\W}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})} - \alpha \cdot \log \det(\W) + \tfrac{1}{2} \beta \cdot \| \W \|_{F}^{2} 
\end{equation}
where $T$ denotes the batch of triplets, $\alpha, \beta$ are hyper parameters,  $\log \det(\W) \eqdef \log (\det(\W))$ and $\| \W \|_{F}^{2}$ is the Frobenius norm of $\W$. It is comprised of the total hinge loss over all the triplets and a $\log \det$ regularization over $\W$ that assigns a finite high cost when $\W$ is PD and is close to the PD cone boundaries. 
Our objective is to find $\W$ that minimizes $L(\W)$, such that $\W$ is PD ($\W\succ 0$).

We observe that the PD cone is a convex set and that the objective function is a linear sum of convex functions over the PD cone. Hence the optimization over the objective is a convex problem with a single minima which is global.

\subsection{Gradient evaluation}
Next, we evaluate a matrix gradient step  $\frac{\partial {l_t (\W)}}{\partial \W}$ of an arbitrary triplet $t$. We define the linear part of the hinge loss of a triplet $t$ by
\begin{equation}
\lambda_t \eqdef \lambda_t(\W, \vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-}) \eqdef 1-\vec{q}_{t}\T \W \vec{p}_{t}^{+}+\vec{q}_{t}\T\W\vec{p}_{t}^{-}
\end{equation}

We note that $\W$ is PD and therefore symmetric, therefore we will require its gradient to be symmetric by replacing $\W$ with $\tfrac{1}{2}(\W + \W\T)$.

The derivative of the ranking loss is then given by
\begin{equation}
\frac{\partial {l_{\W}^{t}}}{\partial \W} = \tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T  + \Delta\vec{p}_{t}\vec{q}_{t}\T]\cdot {l'}(\lambda_t)
\label{dlossranking}
\end{equation} where $l'(x) \eqdef \frac{\partial {l(x)}}{\partial x}$, $\Delta\vec{p}_{t} \eqdef (\vec{p}_{t}^{-} - \vec{p}_{t}^{+})$

Replacing the linear hinge loss \eqref{hingelt} in \eqref{dlossranking}, we observe that it is not differentiable in $\lambda_t = 0$, but it has a sub gradient matrix

\begin{equation}
\frac{\partial {l_t (\W)}}{\partial \W} = \tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T  + \Delta\vec{p}_{t}\vec{q}_{t}\T] \cdot \textbf{1}(\lambda_t)
\end{equation}
where $\textbf{1}(x)$ is an indicator for $x>0$. Hence it is non-zero when \eqref{hingelt} is positive or zero otherwise. Note that replacing the linear hinge loss by a logarithmic loss would have yield a continues $\operatorname{sigmoid}(\lambda_t)$ instead of $\textbf{1}(\lambda_t)$.  We emphasis that 
$\frac{\partial {l_t (\W)}}{\partial \W}$ is a matrix, which represents the gradient of $l_{\W}^{t}$ with respect to each the elements of $W$.

The matrix gradient of $\tfrac{1}{2} \| \W \|_{F}^{2}$ equals $\W$ and the matrix gradient of  $\log \det(\W)$ equals $\W^{-1}$. Therefore, the matrix gradient of the objective function $L(\W)$ can be easily determined: 
\begin{multline}
\frac{\partial {L (\W)}}{\partial \W} = 
\sum\limits_{t\in T}{\{ [\tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T + \Delta\vec{p}_{t}\vec{q}_{t}\T \} } \cdot \textbf{1}(\lambda_t)] \\- \alpha \cdot \W^{-1} + \beta \cdot \W 
\label{gradMat}
\end{multline}

Taking the gradient of the objective which is the sum of of hinge losses over all the triplets in a batch is a straight-forward gradient descent approach and has already been utilized by \cite{qian}. Its drawback is that it can't ensure that the update following the gradient step will keep $\W$ as positive definite. On \cite{qian} a projection to the PSD cone is applied following each update. 

Next, we introduce our novelty for taking a (block coordinate) gradient step that ensures that if $\W$ before the update is PD, then $\newW$ following the update will be kept as PD. 

\subsection{Block coordinate-descent steps}
He we take a (block coordinate) gradient step that ensures that if $\W$ before the update is PD, then $\newW$ following the update will be kept as PD. Each block coordinate step is taken over a single row and column $k$ of $\W$. An upper limit on the step size is evaluated using the Schur Complement condition for positive definiteness and we take a step with a step size within that boundary. 

We define the update:
\begin{equation}
   \newW = \W +\eta\cdot\mat{G}
   \label{updateEq}
\end{equation}
Where $\mat{G}$ is the update matrix based on a single coordinate $k$, $\eta$ is the step size, $\W$ is the pre-update metric matrix, $\newW$ is the post-update metric matrix. We will discuss next on how to choose the steps size $\eta$ and generate $\mat{G}$ row-column $k$ update such that $\newW$ will result as PD. 


Since PD matrices are symmetric, we require the update $\mat{G}$ of \eqref{updateEq} to be symmetric:
\begin{equation}
\mat{G} = \vec{u}\cdot\vec{e_k}\T + \vec{e_k}\cdot\vec{u}\T
\label{gradMtx}
\end{equation}
where  $\vec{u}$ is a column vector that equals the column $k$ of the (symmetric) gradient matrix of the objective \eqref{gradMat}, $\vec{e_k}$ equals an elementary vector for selecting a column $k$ of a matrix. 

\subsubsection{choosing the step size $\eta$}
Following, using the Schur complement condition for positive definiteness \todo{AddREF!}, we derive an upper limit for the step size. We will show that taking a step size on $\mat{G}$ which is smaller than the upper limit guarantees that the update \eqref{updateEq} will keep $\newW$ inside the PD cone. 

The pre-update matrix $\W$ can be formulated according to the Schur complement notation as: 
\begin{equation}
\W= \left[ \begin{matrix} C & \vec{B\T} \\ \vec{B} & \mat{A} \end{matrix} \right]
\label{schurNotationPreUpdate}
\end{equation}
where $C = \W_{(1,1)}$ (scalar), $\vec{B} = \W_{(2:,1)}$, $\mat{A} = \W_{(2:,2:)}$

We apply the same notation to $\newW$
\begin{equation}
\mat{\newW}= \left[ \begin{matrix} C^* & \vec{B^*}\T \\ \vec{B^*} & \mat{A^*} \end{matrix} \right]
\end{equation}
According to the Schur complement condition for positive definiteness \todo{AddREF!}, $\newW$ is PD iff $\mat{A}^*$ and $C^* - \vec{B}^*\T \mat{A}^{*-1} \vec{B}^*$ are both positive definite:
\begin{equation}
\newW \succ  0 \Leftrightarrow (\mat{A}^* \succ  0, C^* - \vec{B}^*\T \mat{A}^{*-1} \vec{B}^* \succ  0)
\label{schurCondPreliminary}
\end{equation}
We note that $\mat{A}^* = \mat{A}$ because we only update in this step the row-column $k=1$. $\mat{A}$ is a minor of $\W \succ  0$. Resulting with $\mat{A}^*=\mat{A} \succ  0$. Moreover, $C^* - B^*\T A^{*-1} B^*$ is a scalar. Therefore \eqref{schurCondPreliminary} reduces to:
\begin{equation}
\mat{W^*} \succ  0 \Leftrightarrow (C^* - B^*\T A^{-1} B^* >  0)
\label{schurCond}
\end{equation}
Next, we expand \eqref{schurCond} according to \eqref{updateEq} and \eqref{gradMtx} (with $k=1$) yielding a condition for $\newW \succ  0$
\begin{equation}
\begin{array}{ll} 
(\W_{(1,1)} + 2\eta \vec{u}_{(1)})  \\
-(\W_{(2:,1)} + \eta \vec{u}_{(2:)})\T \mat{A}^{-1} (\W_{(2:,1)} + \eta \vec{u}_{(2:)})  & > 0
\end{array}
\label{PDUpdateCondNonSimpl}
\end{equation}
Grouping \eqref{PDUpdateCondNonSimpl} as a quadratic inequality, we get:
\begin{equation}
\begin{array}{ll} 
-(\vec{u}_{(2:)}\T \mat{A}^{-1} \vec{u}_{(2:)})\eta^2 \\
+2(\vec{u}_{(1)} - \vec{u}_{(2:)}\T \mat{A}^{-1}\W_{(2:,1)})\eta\\
+(\W_{(1,1)} - \W_{(2:,1)}\T  \mat{A}^{-1} \W_{(2:,1)}) & > 0
\end{array}
\label{PDUpdateCondQuadFormWithW}
\end{equation}
or according to \eqref{schurNotationPreUpdate}:
\begin{equation}
\begin{array}{ll} 
(\vec{u}_{(2:)}\T \mat{A}^{-1} \vec{u}_{(2:)}) \eta^2 \\
-2(\vec{u}_{(1)} - \vec{u}_{(2:)}\T \mat{A}^{-1}\vec{B})\eta \\
-(C - \vec{B}\T  \mat{A}^{-1} \vec{B}) & < 0
\end{array}
\label{PDUpdateCondQuadForm}
\end{equation}
We note that the condition \eqref{PDUpdateCondQuadForm} always holds for $\eta = 0$ because $\W\succ0$ and therefore the RHS of \eqref{schurCond} is true (after replacing $\W^*$ by $\W$). Hence, it ensures that solving \eqref{PDUpdateCondQuadForm} according to $\eta$ results with an upper limit for $\eta$.  The computational complexity of \eqref{PDUpdateCondQuadForm} is $o(d^2)$ because it is a summation of quadratic terms. 

Summing it up, we derived an upper limit \eqref{PDUpdateCondQuadForm} for the step size of a block coordinate (row-column) step (\eqref{gradMtx} and \eqref{updateEq}) that ensures that the update is PD. The computational complexity for the evaluation of \eqref{PDUpdateCondQuadForm} is $o(d^2)$. Nevertheless, evaluating \eqref{PDUpdateCondQuadForm} requires the evaluation of $\W^{-1}$ and $\mat{A}^{-1}$. The most efficient implementation is to update $\W^{-1}$ following a block coordinate step, and to derive $\mat{A}^{-1}$ from $\W^{-1}$ before taking the next step. 

\subsubsection{Complete the step update}

Next, we demonstrate how to evaluate $\W^{*-1}$ following a coordinate step \eqref{updateEq} and how to evaluate $\mat{A}^{*-1}$ before taking the next coordinate step. Both evaluations will cost a computational complexity of $o(d^2)$.

Evaluating $\W^{*-1}$ can be easily taken using the Sbury matrix identity \todo{addref}. By reformulating \eqref{updateEq} and \eqref{gradMtx} according to the Woodbury matrix identity notation, we can write
\begin{equation}
\mat{W^*} = \W+\mat{\widetilde{G}}
\label{updateEqWDB}
\end{equation}
where
\begin{equation}
\mat{\widetilde{G}} = \mat{U}\mat{C}\mat{V} = \left[ \begin{matrix} \vec{u} & \vec{e_k} \end{matrix} \right] \left[ \begin{matrix} \eta & 0 \\ 0 & \eta \end{matrix} \right] \left[ \begin{matrix} \vec{e_k}\T \\ \vec{u}\T \end{matrix} \right]
\label{gradMtxWDB}
\end{equation}

Utilizing the Woodbury matrix identity result with
\begin{equation}
\begin{array}{lcl}
\W^{*-1} = \\
\W^{-1} - \W^{-1} \mat{U} (\eta^{-1} I_{2x2} + \mat{V} \W^{-1} \mat{U})^{-1} \mat{V} \W^{-1}
\end{array}
\label{InvWwdb}
\end{equation}


Last, we evaluate $\mat{A}^{-1}$ before a coordinate step given $\W$ and $\W^{-1}$, using the Schur complement and its corresponding notation \eqref{schurNotationPreUpdate}:
\begin{equation}
\begin{array}{l}
 \W^{-1} =  \\
 \left[ \begin{array}{cc} s & -s \vec{B}\T \mat{A}^{-1} \\ -(s \vec{B}\T \mat{A}^{-1} )\T & (\mat{A}^{-1} + \mat{A}^{-1} \vec{B} s \vec{B}\T \mat{A}^{-1} ) \end{array}  \right]
\end{array}
\label{BlockInvW}
\end{equation}
where $s$ is a scalar that denotes the Schur Complement:  $s \eqdef (\mat{C}-\vec{B}\T \mat{A}^{-1} \vec{B})$. Hence
\begin{equation}
s = \W^{-1}_{(1,1)}
\label{invWkk}
\end{equation}
\begin{equation}
-s \vec{B}\T \mat{A}^{-1} = -\W^{-1}_{(1,1)} \vec{B}\T \mat{A}^{-1} = \W^{-1}_{(1,2:)}
\end{equation}
\begin{equation}
\vec{B}\T \mat{A}^{-1} = -\frac{\W^{-1}_{(1,2:)}}{\W^{-1}_{(1,1)} }
\end{equation}
\begin{equation}
\mat{A}^{-1}\vec{B} = (\vec{B}\T \mat{A}^{-1})\T
\label{invA_Btrans}
\end{equation}
Assign above at the lower right block of \eqref{BlockInvW} results with
\begin{equation}
 \mat{A}^{-1} + \frac{1}{\W^{-1}_{(1,1)} } \W^{-1}_{(2:,1)} (\W^{-1}_{(2:,1)})\T = \W^{-1}_{(2:,2:)}
\end{equation}

Finally, $A^{-1}$ is extracted to be:
\begin{equation}
\mat{A}^{-1} = \W^{-1}_{(2:,2:)}- \frac{\W^{-1}_{(2:,1)} \W^{-1}_{(1,2:)}}{\W^{-1}_{(1,1)}}
\label{InvA}
\end{equation}
and the computational complexity of \eqref{InvA} is $o(d^2)$

\todo{Discuss numerical stability ?}

All of the above is summarized in Algorithm \ref{alg:comet}

\begin{algorithm}[tb]
   \caption{COMET}
   \label{alg:comet}
\begin{algorithmic}[1]
   \STATE {\bfseries input:} training data, max number of steps, $\alpha$, $\beta$
   \STATE {\bfseries initialize:} 
   \STATE Generate a triplet set $T$, $\W  \leftarrow I_{d \times d}$ , $\W^{-1}  \leftarrow I_{d \times d}$

   \REPEAT 
   \STATE Choose a coordinate $k \in {1..d}$ randomly
   \STATE Evaluate $\mat{A}^{-1}$ using $\W^{-1}$ \eqref{InvA}
   \STATE Evaluate the coordinate step gradient matrix \eqref{gradMtx}
   \STATE Choose the step size, with an upper limit  \eqref{PDUpdateCondQuadForm}
   \STATE Update the metric to $\newW$ \eqref{updateEq}
   \STATE Update the metric inverse to $\newW^{-1}$ \eqref{InvWwdb}
   \UNTIL{max number of steps}
\end{algorithmic}
\end{algorithm}

\subsection{Computational complexity}
Our algorithm is competitively fast. Each step takes a fixed amount of operations. It keeps $\W$ within the PD cone and therefore avoid a costly projection to the PD cone following each step. Here, we evaluate the computational complexity for a single coordinate step. The complexity is comprised of an evaluation of the gradient elements and updating $\W$, $\W^{-1}$ and $\mat{A}^{-1}$.

We evaluate the computational complexity of a single coordinate step \eqref{gradMat}. Each element $\delta_{(i,j)}$ of the gradient matrix \eqref{gradMat} equals
\begin{multline}
\delta_{(i,j)} = \sum\limits_{t\in T}{ [\tfrac{1}{2}[(\vec{q}_{t})_i(\Delta\vec{p}_{t}\T)_j + (\Delta\vec{p}_{t}\T)_i(\vec{q}_{t})_j\T] } \cdot \textbf{1}(\lambda_t)  \\ 
 - \alpha \cdot \W^{-1}_{(i,j)} + \beta \cdot \W_{(i,j)}
\label{gradMatElem}
\end{multline}

If the training data is dense, evaluating the sum in \eqref{gradMatElem} costs $o(|T|)$ operations, where $|T|$ is the number of triplets. Nonetheless, if the training data is sparse, with a sparsity coefficient $\gamma$,  $ 0< \gamma <1 $, then evaluating the sum in \eqref{gradMatElem} will cost an average of $o(\gamma^2 |T|)$ operations, because we can accumulate only the elements that are both non-zeros in $(\vec{q}_{t})_i$ and in $(\Delta\vec{p}_{t}\T)_j  $ (same goes to $(\vec{q}_{t})_j$ and $(\Delta\vec{p}_{t}\T)_i$). Evaluating $\W^{-1}_{(i,j)}$ and $\W_{(i,j)}$ costs $o(1)$ since we always keep the updated versions of $\W$ and $\W^{-1}$. Evaluating all the gradient elements $\delta_{(k,:)}$ in a single row $k$ then costs $o(d\cdot \gamma^2 |T|)$. Accounting for $o(d^2)$ which is the cost of evaluating each of \eqref{InvA}, \eqref{PDUpdateCondQuadForm} and \eqref{InvWwdb}, sums up for a computational complexity cost of $o(\gamma^2 d |T| + d^2)$ per coordinate step.

Finally, the total computational complexity cost of our approach is $o(N \cdot (\gamma d)^2 |T| + N \cdot d^3)$ while taking $N \dot d$ coordinate steps, where N is the number of iterations over all the coordinates. Our empirical experimental results showed that our approach converges for choosing $N$ between 5 to 10. 

With regard to memory usage, holding the triplets costs $o(\gamma d |T|)$ elements, holding $\W$ and $\W^{-1}$ costs $o(d^2)$. The total memory usage is $o(\gamma d |T| + d^2)$
\section{Convergence}
Our method is based on minimizing a strongly convex function using block-coordinate descent. There is a well established body of work showing that when using non-overlapping blocks, block-coordinate descent iterates converge w.h.p. in a linear rate to the optimum value \cite{nesterov2012efficiency,richtarik2014iteration}.
However, the blocks we use in our method are overlapping - for example the $(1,2)$ coordinate of the matrix is a part of both the 1\textsuperscript{st} and the 2\textsuperscript{nd} column-row. We thus make use of a more general convergence result applicable to overlapping blocks, given by \citet{richtarik2013optimal}. 
In their paper, \citet{richtarik2013optimal} give sufficient conditions for linear convergence of overlapping block-coordinate descent for a strongly convex smooth objective.
In order for these conditions to hold, we must slightly modify the objective function $L({\W})$. The objective $L(\W)$ is strongly convex but not smooth, since the gradient of the $\log \det$ term is unbounded near the envelope of the positive definite cone. Let $\tilde{L}({\W}) = L({\W + \kappa I_d})$, where $I_d$ is the $d \times d$ identity matrix, and $\kappa$ is a fixed parameter.
Note that our algorithm can easily minimize $\tilde{L}$, the only difference being that we now need to maintain both $\W^{-1}$ and $(\W+\kappa I_d)^{-1}$, which does not change the asymptotic computational complexity. The additional $\kappa I_d$ term acts as a prior, where we add a Euclidean distance term to the distance we learn. 
We now show that the modified objective $\tilde{L}$ obeys Assumption $4$ and Assumption $5$ of \citet{richtarik2013optimal}.Thereby, according to \citet[Theorems 3 and 7]{richtarik2013optimal}, Algorithm \ref{alg:comet} converges to the optimum value in a linear rate.

\begin{lemma}
Let $\tL=\sum\limits_{t\in T}{l_{\W + \kappa I}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})} - \alpha \cdot \log \det(\W + \kappa I) + \tfrac{1}{2} \beta \cdot \| \W + \kappa I \|_{F}^{2}$, where $l_{\W + \kappa I}$ is either the squared hinge loss or the log-loss, and $\tilde{L}$ is defined over the positive semidefininte cone.
Then $\tL$ is strongly convex, and the following condition holds (Assumption 4, \citet{richtarik2013optimal}): $\tilde{L}$ has Lipschitz gradient with respect to the matrix coordinates: There exist positive constants $L_{ij}$ $1 \leq i \leq d, 1 \leq j \leq d$, such that $| \frac{\partial \tL}{W_{ij}} - \frac{\partial \tilde{L}(\W + t \E_{ij})}{W_{ij}} | \leq L_{ij} |t|$ for all $t \in \mathbb{R}$ such that $\W + t\E_{ij}$ is positive semidefinite, where $\E_{ij}$ is a symmetric matrix with all zeros except ones at the $(i,j)$ and $(j,i)$ coordinates.

%\item Assumption 5, \citet{richtarik2013optimal}: 
%$\tL = \sum_{S \in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite collection of %subsets of matrix index pairs (for example a row-column), and $f_S$ are differentiable convex functions such that $f_S$ depends only on coordinates $(i,j) \in S$.

\end{lemma}

\begin{proof}


The objective $\tL$ is comprised of three terms - (1) the sum of loss terms, (2) the $\log \det$ term, and (3) the Frobenius regularization term. The Frobenius norm term ensures that $\tL$ is at least $\beta$ strongly-convex.

Straightforward computation shows that the terms (1) and (3) have Lipschitz gradients, denoted $L^1_{ij}$ and $L^3_{ij}$ respectively: The Lipschitz constant $L^1_{ij}$ for the gradient of the sum of loss terms is bounded by $\sum_{t \in T} (\vec{{q_t}}_i {\Delta \vec{p_t}}_j)^2 + (\vec{{q_t}}_j {\Delta \vec{p_t}}_i)^2$. Assuming the data is normalized such that the maximum entry if smaller than $1$, we have $L^1_{ij} \leq 2 T$.

The Lipschitz constant $L^3_{ij}$ for the Frobenius regularization is immediately seen to be $\beta$.

In order to show that the $\log \det$ function has Lipschitz derivatives in our domain, we bound the diagonal entries of its Hessian matrix. The Hessian diagonal elements serve as Lipschitz bounds, which can be seen using standard Taylor expansion on the difference $\frac{\partial \tL}{W_{ij}} - \frac{\partial \tilde{L}(\W + t \E_{ij})}{W_{ij}}$.

The Hessian matrix $\mathcal{H}$ of the $- \log \det$ function is a $d^2 \times d^2$ matrix, whose entries are indexed by pairs $((i,j),(k,l))$, such that the diagonal entries are indexed by $(i,j),(i,j)$. The Hessian itself is cumbersome to compute, however computing its diagonal terms can be done relatively easily using the following identity for the Hessian of the $-\log \det$ function evaluated at a point $X$ \citep{boyd2004convex}:  
\begin{align*}
\mathcal{H}_{(i,j),(i,j)} = tr\left(X^{-1}E_{ij} X^{-1}E_{ij}\right)  \\
= 2 \left((X^{-1}_{ij})^2 + X^{-1}_{ii}X^{-1}_{jj}\right).
\end{align*}
In our case we have $X = \W+\kappa I$, where $\W$ is PSD and $\kappa >0$. Thus, $X$ and $X^{-1}$ are positive definite, and we have $(X^{-1}_{ij})^2 \leq X^{-1}_{ii}X^{-1}_{jj}$. We therefore have:
\begin{align}
\label{eq:hes}
L^2_{ij} \leq  \mathcal{H}_{(i,j),(i,j)} \leq 4 X^{-1}_{ii}X^{-1}_{jj} \leq \nonumber \\
4 \|X^{-1}\|^2 = \frac{4}{\|X\|^2} \leq \frac{4}{\kappa^2},
\end{align}
where the norm in \ref{eq:hes} is the operator norm (maximum singular value), and the last inequality is due to the fact that $\W$ is PSD, and $\|X\| = \| \W + \kappa I\| \geq \kappa$.

Collecting all the terms together, we obtain an overall bound on the Lipschitz constant $L_{ij} \leq L^1_{ij} + L^2_{ij} + L^3_{ij} \leq 2 T + \beta + \frac{4 \alpha}{\kappa ^2}.$

%\item Assumption 5, \citet{richtarik2013optimal}: 
%$\tL = \sum_{S \in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite %collection of subsets of matrix index pairs (for example a row-column), and $f_S$ are differentiable convex functions such that $f_S$ depends only on coordinates $(i,j) \in S$.


\end{proof}

\begin{theorem}
Let $\W^k$ be the $k$-th iterate of Algorithm \ref{alg:comet} with objective function $\tL$, and let $\tilde{L}^*$ be the optimal value of $\tilde{L}$ on the PSD cone.
If $k >$ \todo{compute what should be here}, then $Prob(\tilde{L}(\W^k) - \tilde{L}^* \leq \epsilon) \geq 1-\rho$.
\end{theorem}
\begin{proof}
proof here
\end{proof}
\section{Experiments}
We evaluate COMET on real world several datasets and compare its performance with several other approaches. All the approaches we compare with learn a Mahalanobis metric matrix and use coordinate descent method that ensures that each update of the metric matrix results inside the PD cone. 


\subsection{Experimental setup}

\subsection{Competing approaches}



\textbf{COMET} - The algorithm described above.

\textbf{Euclidean} - The standard Euclidean distance in the feature space. The initialization of COMET using the identity matrix is equivalent to this distance measure.

\textbf{HDSL} - is an approach that deals with high-dimensional sparse data \cite{hdsl}. It is comprised on rank-1 sparse update matrices that are all zeros except for $2\times2$ pair of features elements. It greedily incorporates one pair of features at a time to control the number of active features.

\textbf{LEGO} - learns a Mahalanobis distance in an online fashion using a regularized per instance loss yielding a positive semidefinite matrix \cite{lego}. The main variant of LEGO aims to fit a given set of pairwise distances. We used another variant of LEGO that, like COMET, learns from relative distances. In our experimental setting, the loss is incurred for same-class samples being more than a certain distance away, and different class samples being less than a certain distance away.

\textbf{BoostMetric} - is based on the observation that any positive semidefinite matrix can be decomposed into linear positive combination of trace-1 rank-1 matrices  \cite{boost}. It uses rank-1 positive semidefinite matrices as weak learners within a boosting based learning process.

%OASIS??

\subsection{Datasets}
We evaluate COMET on several datasets real world datasets of different sizes and dimensionality. In all the datasets we examine, two samples are considered similar if they share the same class label. Each data set is tested on a two layer 5 fold cross validation experiment with 80\%/20\% random splits. We use the same random splits across all approaches. We trained all the approaches with the exact same set of triplets, except for LEGO that uses pairs constraints and we've ensured that we choose triplets/constraints number in a regime that test performance converges. We generated the triplets randomly with a constant number of triplets per sample.

\textbf{Reuters CV1} is a text classification dataset, with \textit{bag-of-words} representation. We take a 4 classes subset of Reuters CV1 introduced in \cite{CaiRCV14} and was tested for metric learning in \cite{hdsl}. We selected a subset of 5000 features that conveyed high information about the identity of the class using the \textit{infogain} criterion \cite{infogain}. This is a discriminative criterion, which measures the number of bits gained for category prediction by knowing the presence or absence of a term in a document. The selected features with normalized using \textit{tf-idf} and then represented each document as a bag of words.  
We used 100,000 triplets (and 200,000 LEGO constraints) for training Reuters CV1 which is the same scale that was used in \cite{hdsl}. For training HDSL, we took 8000 iterations as was taken by \cite{hdsl}

\textbf{Caltech256} is an image classification dataset.   We take a 50 classes subset of Caltech256 introduced and tested for metric learning in \cite{oasis}. It contains 65 images per class (total of 3250 images), represented with 975 \textit{bag-of-local-descriptors} features. We used 135,000 triplets (and 300,000 LEGO constraints) for training Reuters CV1 which is the same scale that was used in \cite{oasis}. We used early stopping after 8000 iterations to train HDSL. BoostMetric runs very slow and uses a hugh amount of memory. Therefore, we took the number of COMET coordinate steps to be the maximal number of BoostMetric rank1 updates

%\textbf{Semeion} is handwritten digits image dataset with binary sparse representation. That was downloaded from the UCI repository \cite{UCI, semeion} and was tested for metric learning in \cite{qian}. It contains 1593 images, each image is represented sparsely by 256 binary pixels features.

\textbf{Protein} is a bioinformatics dataset that was downloaded from the LIBSVM repository \cite{libsvm} and was tested for metric learning  in \cite{qian}. It cons 24387 samples, each sample is represented by 357 features. 

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{COMET_convergence}}
\caption{ \textit{Precision-at-top-k} of COMET, evaluated on the test set, as a function of the number of coordinate steps, normalized to the data dimension. Error bars are standard error of the means across 5 random train/test partitions (80\%/20\%) }
\label{cometConvergeFig}
\end{center}
\vskip -0.2in
\end{figure} 

\subsection{Evaluation Measures}
We evaluated the performance of all algorithms using standard ranking precision measures
based on nearest neighbors. For each query image in the test set, all other test images were
ranked according to their similarity to the query image. The number of same-class images
among the top k images (the k nearest neighbors) was computed. When averaged across test
images (either within or across classes), this yields a measure known as \textit{precision-at-top-k},
providing a precision curve as a function of the rank k.

\subsection{Results}
 We evaluated the \textit{precision-at-top-k} on the test set, as a function of $k$ neighbours and averaged the results across 5 random train/test partitions (80\%/20\%).
Figure \ref{cometConvergeFig} traces the \textit{precision-at-top-k} over the test sets as it progresses during learning. We observe that convergence is usually achieved after (6 to 8)$\times d$ coordinate steps.
Figure \ref{precFig} compare the precision obtained with COMET, with four competing approaches, as described above. COMET achieved consistently superior or equal results throughout the full range of k (number of neighbours) tested. 


\todo{mention frob norm hasn't improved performance}
%  Note that we've also tested our approach with Frobenius norm regularization, but haven't seen a significant improvement for utilizing it.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Precision_at_K_all_datasets}}
\caption{ (best seen in color) Comparison of the performance of COMET, LEGO, BoostMetric, HDSL and the Euclidean metrics across the datasets . Each curve shows the \textit{precision-at-top-k}, evaluated on the test set, as a function of $k$ neighbours. The results are averaged across 5 train/test random partitions (80\%/20\%), error bars are standard error of the means (s.e.m.) }
\label{precFig}
\end{center}
\vskip -0.2in
\end{figure} 

\section{Summary}
This is the best paper ever.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{comet}
\bibliographystyle{icml2015}

\end{document} 

