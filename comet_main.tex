%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% COMET ICML 2015 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

%% COMET packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

%%%%%%%%%%%%%%%%%%%%%%%%


% use Times
\usepackage{times}
\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Metric learning using Block Coordinate Descent}
\begin{document} 

\twocolumn[
\icmltitle{Metric Learning using Block Coordinate Descent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Yuval Atzmon}{yuval.atzmon@biu.ac.il}
\icmladdress{The Gonda Brain Research Center, Bar Ilan University, 52900, Israel}
\icmlauthor{Uri Shalit}{ \todo{Uris email}}
\icmladdress{\todo{uri affiliation}}
\icmlauthor{Gal Chechik}{ \todo{gal.chechik@biu.ac.il}}
\icmladdress{The Gonda Brain Research Center, Bar Ilan University, 52900, Israel}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{metric learning, coordinate descent, similarity learning}

\vskip 0.3in
]

%%%% COMET commands %%%%
\newcommand\todo[1]{\textbf{<ToDo:#1}!}
%\newcommand\mat[1]{\mathcal{#1}}
%\newcommand\mat[1]{\boldmath{#1}}
\newcommand\mat[1]{{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\newcommand{\T}{{}^\mathsf{T}}
\newcommand{\W}{\mat{W}}
\newcommand{\E}{\mat{E}}
\newcommand{\Hh}{\mat{H}}
\newcommand{\newW}{{\mat{W^*}}}
\newcommand{\eqdef}{\doteq}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tL}{\tilde{L}(\W)}
\newcommand{\frob}[1]{{\|#1\|_F^2}} 
\newcommand{\ignore}[1]{}

\newcommand{\q}{{\vec{q}}}
\newcommand{\p}{{\vec{p}}}
\newcommand{\trip}{{t}}
\newcommand{\qt}{{\q_{\trip}}}
\newcommand{\pt}{{\p_{\trip}}}

\newcommand{\grd}{\frac{\partial \tL}{\W}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

%\DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} 
TBD
\end{abstract} 

\section{Introduction}
Learning a measure of pair-wise similarity or distance among data samples is a fundamental task in machine learning. Learned metrics can be used to retrieve a similar image or document given an example \cite{}, or be used as a representation for a supervised learning technique that is based on distances, like nearest-neighbors or kernel methods \cite{}.


\section{Related work}
There has been a lot of work on learning distance metrics and similarity measures from data, see \cite{bellet2013survey, kulis2012survey, yang2006survey} for surveys. Specifically, several approaches have been proposed for learning a positive definite (PD) Mahalanobis matrix. Learning a PD matrix is a difficult task because it requires maintaining all eigen-values positive (or at least non-negative) during the optimization process. 

One family of approached views learning Mahalanobis distances, as learning a linear projection of the data into another space (often of lower dimensionality), where a Euclidean distance is
defined among pairs of objects. Such approaches include Fisherâ€™s Linear Discriminant Analysis
(LDA), relevant component analysis (RCA) (\todo{Bar-Hillel et al., 2003)}, supervised global
metric learning \todo{(Xing et al., 2003)}, large margin nearest neighbor (LMNN) \todo{(Weinberger
et al., 2006)} and Metric Learning by Collapsing Classes \todo{(Globerson and Roweis, 2006)}.

Another family of approaches use an information measure for the optimization problem. Most notably are Information-Theoretic Metric Learning \todo{
Davis et al. (2007)} which introduced LogDet divergence regularization and LEGO \cite{lego} based on work by \todo{Davis et al. (2007)}, which learns a linear model of a [dis-]similarity function between documents in an online way.

A third family of approaches \cite{boost} \todo{cite (Bi et al. (2011), Liu and Vemuri (2012) } learn the metric matrix using rank-1 (PSD) updates which are generated by solving a dual optimization problem within a boosting-based learning process \todo{cite AdaBoost}.

\todo{Talk about bilinear similarity learning (OASIS/POLA/..)}

Recently, new approaches have been suggested to learn metrics at regimes of high dimensional sparse data. Notably \cite{hdsl} is comprised on rank-1 sparse update matrices that are all zeros except for $2\times2$ pair of features elements. It greedily incorporates one pair of features at a time to control the number of active features. \cite{qian2014} introduces randomized low rank matrix approximation and adaptive sampling of the constraints which is was developed on \cite{qian}. \cite{qian} however, still requires a costly projection step onto the PSD cone after every gradient step which scales in $o(d^3)$ due to the eigenvalue decomposition.

\todo{Talk about schur complement and row-by-row work from 2009 ,\cite(rbr))}

% ===============================================
% START NEW VERSION
\section{The learning setup}
We address the problem of learning a distance metric over a set of
entities, like images or text documents based on data on their
relative similarities.

Formally, following \cite{OASIS}, let \cal{P} be a set of entities
$\{\p_1,...,\p_N\}$ each represented as a vector in $\Rd$.  We measure
the similarity of two samples $\q, \p \in \cal{P}$ using a bilinear
form parametrized by a model $\W \in \mathbb{R}^{d \times d}$.
\begin{equation}
  S_{\W}(\q, \p) = \q\T \W \p
\end{equation}
\newline
{\bf{XXX Yuval, you need to explain how the similarity is later used a sa
distance measure (since W is positive, one can project using sqrtm(W)
and use euclidean distance in the projected space  XXX }}
\newline

We assume that a weak form of supervision is given in the form of
ranking relation over triplets. This form of supervision is often easy
to obtain and has been shown to achieve good performance
\cite{lmnn,oasis,qian}. Specifically, we assume we have access to
triplets of entities from $\cal{P}$, where each triplet $t$ consists of
a ``query'' instance $\qt \in \cal{P}$, and two instance $\pt^+,
\pt^- \in \cal{P}$ such that $\qt$ is more similar to $\pt^+$
than to $\pt^-$.

We aim to find a similarity measure $S_{\W}$ that agrees with the
ranking of these triplets, namely, $S_{\W}(\q, \p^+) > S_{\W}(\q,
\p^-)$. To achieve this, we define a hinge loss for a triplet
\begin{equation}
  l_{\W}(\qt, \pt^+, \pt^-) = [1-\qt\T\W\pt^+ + \qt\T\W\pt^-]_{+}
  \label{single-triplet-hinge-loss}
\end{equation}

where $[z]_{+} \eqdef max(0,z)$.  Given a batch of $T$ triplets, and
adding a Frobenius regularization term, we therefore wish to solve the
following regularized optimization problem
\begin{eqnarray}
  \min_{\W}& \sum_{\trip=1}^T  l_{\W}(\qt, \pt^+, \pt^-) + \beta \frob{\W}
 \\  \nonumber
   \rm{s.t.}& \W \succ 0
  \label{hingelt}
\end{eqnarray}
where $\frob{\W}$ is the Frobenius norm of the matrix $\W$, $\beta$ is
the regularization weight and $\W \succ 0$ refers to $\W$ being
positive definite. This optimization problem is convex in $\W$ since
the objective is a sum of convex functions in $\W$, and optimization
is over the cone of positive definite matrices which is convex.

In several previous metric learning approaches \cite{pola, others},
the constrained optimization problem is solved in an online way by
making stochastic gradient steps, while repeatedly projecting back to
the convex cone of positive definite matrices. This projection amounts
to solving an eigen-decomposition problem and is therefore costly in
run-time.

An alternative approach is to use interior-point methods. For example,
avoid the costly projection by adding a log-barrier term, as in
\cite{itml}
\begin{equation}
  \min_{\W} \sum_{\trip=1}^T  l_{\W}(\qt, \pt^+, \pt^-) + \alpha \log \det(\W) + \beta \frob{\W}
\label{eq-logdet-loss}
\end{equation}
where $\alpha$ is a hyper-parameter that determines the weight of the
logdet regularizer, and the $\log \det$ term ensures that only
positive definite matrices are obtained when minimizing
\eqref{eq-logdet-loss}.



% ===============================================================
\section{COMET: Coordinate-descent metric learning}
The optimization problem of \eqref{eq-logdet-loss} can be solved using
stochastic gradient descent (SGD), by iteratively computing the
gradient w.r.t. $\W$ for a set of triplets
\begin{multline}
  \frac{\partial {L (\W)}}{\partial \W} = \sum\limits_{t\in T}{\{
  [\tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T \} }
  \textbf{1}(l_t(\W))] \\- \alpha \W^{-1} + \beta \W
  \label{gradMtx}
\end{multline}
where $\Delta\p_t = \p_t^+ - \p_t^-$, and $\textbf{1}(l_t(\W))$ is an
indicator function which obtains a value of one if the loss is
positive, and zero otherwise (see appendix for derivation details).

This formulation suffers from several drawbacks. First, computing the
inverse of $\W$ is costly, so naive implementations of this approach
are slow. Second, using SGD relies on the $\log det$ term to keep the
current $\W$ inside the PD cone, but it may distort the gradients near
the border of the convex set. This could be harmful when the optimum
lies on the boundary. Third, in matrix learning problems the
optimization dimension is typically very large because it is quadratic
with the feature dimension. At the same time computing the gradient
may actually be wasteful, since step in a approximated direction may
be good enough and faster to compute.

We propose an algorithm that alleviates these problems by using
block-coordinate descent. It makes it easy to keep the search over
$\W$ within the boundaries of the positive definite cone, and allows
to compute the update steps efficiently.

Coordinate descent is {\bf{BLA BLA}} explain some potential benefit
\cite{}. ... \todo{Uri, could you take a pass?}

Our algorithm applies block-coordinate-descent approach as follows: At
each step, a single feature is drawn uniformly at random, all the
matrix entries on its row and columns are treated as a block, and
all being updated. Importantly, we compute analytically a bound on the
size of the update step, which guarantees that the updated matrix
remains positive definite.

More formally, let $k$ be the index of a randomly-drawn feature. We
compute the gradient from \eqref{gradMtx} but only over entries from
the column $k$ and the row $k$ of $\W$ (total of $sd-1$ entries).
Since $\W$ is PD it is also symmetric, we can replace $\W$ in
\eqref{eq-logdet-loss} with $\tfrac{1}{2}(\W + \W\T)$.  This
guarantees that the gradient resulting from \eqref{gradMtx} is also
symmetric see details of derivation in appendix A. We Then define the
matrix $G$ to be a matrix that is all zeros except the values in the
$k^{th}$ row and column, and update the weight matrix 
$\newW = \W^t +\eta G$.
%   \label{updateEq}


{\ignore{
Since PD matrices are symmetric, we require the update $\mat{G}$ of \eqref{updateEq} to be symmetric:
\begin{equation}
  \mat{G} = \vec{u}\cdot\vec{e_k}\T + \vec{e_k}\cdot\vec{u}\T
  \label{gradMtx}
\end{equation}
where $\vec{u}$ is a column vector that equals the column $k$ of the
(symmetric) gradient matrix of the objective \eqref{gradMat},
$\vec{e_k}$ equals an elementary vector for selecting a column $k$ of
a matrix.
}

\subsection{Selecting the step size $\eta$}
Taking a coordinate step may take $\newW$ out of the PD cone. We now
show how to bound the step size to guarantee that $\newW$ remains PD
using Schur complement condition for positive definiteness.

Without loss of generality, lets assume that the current round updates
the first feature ($k = 1$).  We then write the pre- and post-update
matrices, as
\begin{equation}
  \W = \left[ \begin{matrix} C & B\T \\ B & A \end{matrix} \right]
  \quad
  \newW = \left[ \begin{matrix} C^* & B^*\T \\ B^* & A^* \end{matrix} \right]
  \label{schurNotationPreUpdate}
\end{equation}
where $C = \W_{(1,1)} \in \R$ (a scalar), $B = \W_{(2:d,1)} \in
\R^{d-1}$ (a column vector) and $A = \W_{(2:d,2:d)} \in \R^{(d-1)
  \times (d-1)}$. Similarly for $A^*$, $B^*$ and $C^*$.

According to the Schur complement condition for positive definiteness
\citep[p. 650]{boyd2004convex}, $\newW$ is PD if and only if both
$A^*$ and $C^* - B^*\T A^{*-1} B^*$ are positive definite:
%\begin{equation}
%  \newW \succ  0 \Leftrightarrow (A^* \succ  0, C^* - B^*\T A^{*-1} B^* \succ 0)
%  \label{schurCondPreliminary}
%\end{equation}
Given that $W \succ 0$, that $A$ is a minor of $\W$, and that the
current step only updates the row and column of $k=1$, we have $A^* =
A \succ 0$.  Moreover, $C^* - B^*\T A^{*-1} B^*$ is a
scalar, and \eqref{schurCondPreliminary} reduces to:
\begin{equation}
  \mat{W^*} \succ  0 \Leftrightarrow  C^* - B^*\T A^{-1} B^* >  0
  \label{schurCond}
\end{equation}
%

% Yuval, I'm suggesting a simpler notation here. WDYT?
\newcommand{\uscalar}{u_{(1)})}
\newcommand{\uvec}{\vec{u}_{(2:d)})} 
\newcommand{\Wvec}{\W_{(2:d,1)}}
% \newcommand{\uscalar}{v}
% \newcommand{\uvec}{\vec{u}} 
% \newcommand{\Wvec}{\W_{(2:d,1)}}

%
Now let $\uscalar = C^*-C$ and $\uvec = B^* - B$ be the update vectors
and scalar from $G = \newW - \W$. We expand \eqref{schurCond} and
\eqref{gradMtx} (with $k=1$) yielding a condition for $\newW \succ 0$
\begin{equation}
  \begin{array}{ll} 
    (\W_{(1,1)} + 2\eta \uscalar) \\
    -(\W_{(2:d,1)} + \eta \uvec)\T \mat{A}^{-1} (\W_{(2:d,1)} + \eta \uvec)  & > 0
  \end{array}
  \label{PDUpdateCondNonSimpl}
\end{equation}
Grouping \eqref{PDUpdateCondNonSimpl} as a quadratic inequality, we obtain
\begin{equation}
  \begin{array}{ll} 
    -(\uvec\T \mat{A}^{-1} \uvec) \eta^2 \\
    +2(\uscalar - \uvec\T \mat{A}^{-1} \Wvec)\eta \\
    +(\W_{(1,1)} - \Wvec \T  \mat{A}^{-1} \Wvec) & > 0
  \end{array}
  \label{PDUpdateCondQuadFormWithW}
\end{equation}
or according to \eqref{schurNotationPreUpdate}
\begin{equation}
  \begin{array}{ll} 
    (\uvec\T \mat{A}^{-1} \uvec) \eta^2 \\
    -2(\uscalar - \uvec\T \mat{A}^{-1}\vec{B})\eta \\
    -(C - B\T  \mat{A}^{-1} B) & < 0 \quad .
  \end{array}
  \label{PDUpdateCondQuadForm}
\end{equation}
For $\eta = 0$ the condition in \eqref{PDUpdateCondQuadForm} always
holds since $\W \succ 0$ guarantees that $C-B|TA^-1>0$. As a result,
solving \eqref{PDUpdateCondQuadForm} for $\eta$ always has a feasible
solution. This solution provides an upper bound on $\eta$ that
guarantees that $\newW$ is PD.  The computational complexity of
solving \eqref{PDUpdateCondQuadForm} (assuming $A^-1$ is given) is
$O(d^2)$ because computing the coefficients involves computing
bilinear terms.

Furthermore, computing $\newW^{-1}$ and $\mat{A^*}^{-1}$ can be done
efficiently given $\W^{-1}$ and $\mat{A}^{-1}$. Appendix
\ref{appendix-inverse} shows that this can be done in $O(d^2)$
operations.


To conclude, we derived an upper limit \eqref{PDUpdateCondQuadForm}
for the step size of a block coordinate (row-column) step
(\eqref{gradMtx} and \eqref{updateEq}) that guarantees that the
updated matrix is PD. The computational complexity for the evaluation
of \eqref{PDUpdateCondQuadForm} is $o(d^2)$, while using \todo{HOW MUCH MEMORY?}.

\todo{Discuss numerical stability ?}

All of the above is summarized in Algorithm \ref{alg:comet}

\begin{algorithm}[tb]
   \caption{COMET}
   \label{alg:comet}
\begin{algorithmic}[1]
   \STATE {\bfseries input:} training data, max number of steps, $\alpha$, $\beta$
   \STATE {\bfseries initialize:} 
   \STATE Generate a triplet set $T$, $\W  \leftarrow I_{d \times d}$ , $\W^{-1}  \leftarrow I_{d \times d}$

   \REPEAT 
   \STATE Select a coordinate $k \in {1..d}$ uniformly at random.
   \STATE Compute $\mat{A}^{-1}$ using $\W^{-1}$. \eqref{InvA}
   \STATE Compute the coordinate step gradient matrix $G$. \eqref{gradMtx}
   \STATE Select the step size $\eta$, with an upper limit from \eqref{PDUpdateCondQuadForm}
   \STATE Update the metric to $\newW$. \eqref{updateEq}
   \STATE Update the metric inverse to $\newW^{-1}$. \eqref{InvWwdb}
   \UNTIL{max number of steps}
\end{algorithmic}
\end{algorithm}


% END NEW VERSION
% ===============================================

\subsection{Analysis of computational complexity}
COMET is competitively fast. Each step takes a fixed amount of operations. It keeps $\W$ within the PD cone and therefore avoid a costly projection to the PD cone following each step. Here, we evaluate the computational complexity for a single coordinate step. The complexity is comprised of an evaluation of the gradient elements and updating $\W$, $\W^{-1}$ and $\mat{A}^{-1}$.

We evaluate the computational complexity of a single coordinate step \eqref{gradMat}. Each element $\delta_{(i,j)}$ of the gradient matrix \eqref{gradMat} equals
\begin{multline}
\delta_{(i,j)} = \sum\limits_{t\in T}{ [\tfrac{1}{2}[(\vec{q}_{t})_i(\Delta\vec{p}_{t}\T)_j + (\Delta\vec{p}_{t}\T)_i(\vec{q}_{t})_j\T] } \cdot \textbf{1}(\lambda_t)  \\ 
 - \alpha \cdot \W^{-1}_{(i,j)} + \beta \cdot \W_{(i,j)}
\label{gradMatElem}
\end{multline}

If the training data is dense, evaluating the sum in \eqref{gradMatElem} costs $o(|T|)$ operations, where $|T|$ is the number of triplets. Nonetheless, if the training data is sparse, with a sparsity coefficient $\gamma$,  $ 0< \gamma <1 $, then evaluating the sum in \eqref{gradMatElem} will cost an average of $o(\gamma^2 |T|)$ operations, because we can accumulate only the elements that are both non-zeros in $(\vec{q}_{t})_i$ and in $(\Delta\vec{p}_{t}\T)_j  $ (same goes to $(\vec{q}_{t})_j$ and $(\Delta\vec{p}_{t}\T)_i$). Evaluating $\W^{-1}_{(i,j)}$ and $\W_{(i,j)}$ costs $o(1)$ since we always keep the updated versions of $\W$ and $\W^{-1}$. Evaluating all the gradient elements $\delta_{(k,:)}$ in a single row $k$ then costs $o(d\cdot \gamma^2 |T|)$. Accounting for $o(d^2)$ which is the cost of evaluating each of \eqref{InvA}, \eqref{PDUpdateCondQuadForm} and \eqref{InvWwdb}, sums up for a computational complexity cost of $o(\gamma^2 d |T| + d^2)$ per coordinate step.

Finally, the total computational complexity cost of our approach is $o(N \cdot (\gamma d)^2 |T| + N \cdot d^3)$ while taking $N \dot d$ coordinate steps, where N is the number of iterations over all the coordinates. Our empirical experimental results showed that our approach converges for choosing $N$ between 5 to 10. 

With regard to memory usage, holding the triplets costs $o(\gamma d |T|)$ elements, holding $\W$ and $\W^{-1}$ costs $o(d^2)$. The total memory usage is $o(\gamma d |T| + d^2)$


\section{Convergence}
Our method is based on minimizing a strongly convex function using block-coordinate descent. There is a well established body of work showing that with non-overlapping blocks, block-coordinate descent iterates converge w.h.p. in a linear rate to the optimum value \cite{nesterov2012efficiency,richtarik2014iteration}.
However, the blocks we use in our method are overlapping - for example the $(1,2)$ coordinate of the matrix is a part of both the 1\textsuperscript{st} and the 2\textsuperscript{nd} column-row. We thus make use of a much more general convergence result applicable to overlapping blocks, given by \citet{richtarik2013optimal}. 
In their paper, \citet{richtarik2013optimal} give sufficient conditions for linear convergence of overlapping block-coordinate descent for a strongly convex smooth objective.
In order for these conditions to hold, we must slightly modify the objective function $L({\W})$. The objective $L(\W)$ is strongly convex but not smooth, since the gradient of the $\log \det$ term is unbounded near the envelope of the positive definite cone. Let $\tilde{L}({\W}) = L({\W + \kappa I_d})$, where $I_d$ is the $d \times d$ identity matrix, and $\kappa$ is a fixed parameter.
Note that our algorithm can easily minimize $\tilde{L}$, the only difference being that we now need to maintain both $\W^{-1}$ and $(\W+\kappa I_d)^{-1}$, which does not change the asymptotic computational complexity. The additional $\kappa I_d$ term acts as a prior, where we add a Euclidean distance term to the distance we learn. 

We show that the modified objective $\tilde{L}$ obeys Assumptions $1$ and $2$ of \citet{richtarik2013optimal}. Thereby, according to \citet[Theorem 3]{richtarik2013optimal}, Algorithm \ref{alg:comet} converges to the optimum value in a linear rate.

\begin{lemma}[Smooth objective]

Let $\tL=\sum\limits_{t\in T}{l_{\W + \kappa I}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})} - \alpha \cdot \log \det(\W + \kappa I) + \tfrac{\beta}{2}  \cdot \| \W + \kappa I \|_{F}^{2}$, where $l_{\W + \kappa I}$ is either the squared hinge loss or the log-loss, and $\tL$ is defined over the positive semidefininte cone. 
Let $\Hh^i \in \R^{d \times d}$, $i=1 \ldots d$, be a symmetric matrix with non-zero entries only on the $i$-th row and column.

For any $\W$ and $\Hh^i$ such that $\W + \Hh^i$ is PSD, there exists a positive constant $L_i$ such that:
\begin{equation}
\tilde{L}(\W + \Hh^i) \leq \tL + \langle \grd, \Hh^i \rangle + \frac{L_i}{2} \|\Hh^i\|_F^2
\end{equation}

%Then $\tL$ is strongly convex, and the following condition holds (Assumption 4, \citet{richtarik2013optimal}): $\tilde{L}$ has Lipschitz gradient with respect to the matrix coordinates: There exist positive constants $L_{ij}$ $1 \leq i \leq d, 1 \leq j \leq d$, such that $| \frac{\partial \tL}{W_{ij}} - \frac{\partial \tilde{L}(\W + t \E_{ij})}{W_{ij}} | \leq L_{ij} |t|$ for all $t \in \mathbb{R}$ such that $\W + t\E_{ij}$ is positive semidefinite, where $\E_{ij}$ is a symmetric matrix with all zeros except ones at the $(i,j)$ and $(j,i)$ coordinates.

%\item Assumption 5, \citet{richtarik2013optimal}: 
%$\tL = \sum_{S \in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite collection of %subsets of matrix index pairs (for example a row-column), and $f_S$ are differentiable convex functions such that $f_S$ depends only on coordinates $(i,j) \in S$.
\end{lemma}

\begin{proof}

The objective $\tL$ is comprised of three terms - (1) the sum of loss terms, (2) the $\log \det$ term, and (3) the Frobenius regularization term. We will bound each of the separately, denoting the positive boudning constants $L^1_i$, $L^2_i$ and $L^3_i$, respectively.  %The Frobenius norm term ensures that $\tL$ is at least $\beta$ strongly-convex.

Assuming the instances $\qt$ and $\pt$ are unit normalized, straightforward computation shows that for the term (1), the inequality holds true for $L^1_i \leq 2 \sum_{t=1}^T \qt_i^2 +{\Delta\vec{p}_{t}}_i^2 \leq 4T$.

The constant $L^3_{i}$ for the Frobenius regularization is immediately seen to be $\beta$.


In order to show that the $\log \det$ function is smooth in our domain, we bound the maximal eigenvalue of its Hessian. This suffices, since for twice differentiable functions the maximum Hessian eigenvalue bounds the smoothness.

%In order to show that the $\log \det$ function is smooth in our domain, we bound the diagonal entries of its Hessian matrix. The Hessian diagonal elements serve as Lipschitz bounds, which can be seen using standard Taylor expansion on the difference $\frac{\partial \tL}{W_{ij}} - \frac{\partial \tilde{L}(\W + t \E_{ij})}{W_{ij}}$.

The Hessian matrix $\mathcal{H}$ of the $- \log \det$ function is a $d^2 \times d^2$ matrix, whose entries are indexed by pairs $((i,j),(k,l))$, such that the diagonal entries are indexed by $(i,j),(i,j)$. The Hessian itself is cumbersome to compute, however computing its diagonal terms can be done relatively easily using the following identity for the Hessian of the $-\log \det$ function evaluated at a point $X$ \citep{boyd2004convex}:  
\begin{align*}
\mathcal{H}_{(i,j),(i,j)} = tr\left(X^{-1}E_{ij} X^{-1}E_{ij}\right)  \\
= 2 \left((X^{-1}_{ij})^2 + X^{-1}_{ii}X^{-1}_{jj}\right).
\end{align*}
In our case we have $X = \W+\kappa I$, where $\W$ is PSD and $\kappa >0$. Thus, $X$ and $X^{-1}$ are positive definite, and we have $(X^{-1}_{ij})^2 \leq X^{-1}_{ii}X^{-1}_{jj}$. We therefore have:
\begin{align}
\label{eq:hes}
L^2_{ij} \leq  \mathcal{H}_{(i,j),(i,j)} \leq 4 X^{-1}_{ii}X^{-1}_{jj} \leq \nonumber \\
4 \|X^{-1}\|^2 = \frac{4}{\|X\|^2} \leq \frac{4}{\kappa^2},
\end{align}
where the norm in \ref{eq:hes} is the operator norm (maximum singular value), and the last inequality is due to the fact that $\W$ is PSD, and $\|X\| = \| \W + \kappa I\| \geq \kappa$.

Collecting all the terms together, we obtain an overall bound on the Lipschitz constant $L_{ij} \leq L^1_{ij} + L^2_{ij} + L^3_{ij} \leq 2 T + \beta + \frac{4 \alpha}{\kappa ^2}.$

%\item Assumption 5, \citet{richtarik2013optimal}: 
%$\tL = \sum_{S \in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite %collection of subsets of matrix index pairs (for example a row-column), and $f_S$ are differentiable convex functions such that $f_S$ depends only on coordinates $(i,j) \in S$.


\end{proof}

\begin{theorem}
Let $\W^k$ be the $k$-th iterate of Algorithm \ref{alg:comet} with objective function $\tL$, and let $\tilde{L}^*$ be the optimal value of $\tilde{L}$ on the PSD cone.
If $k >$ \todo{compute what should be here}, then $Prob(\tilde{L}(\W^k) - \tilde{L}^* \leq \epsilon) \geq 1-\rho$.
\end{theorem}
\begin{proof}
proof here
\end{proof}
\section{Experiments}
We evaluate COMET on real world several datasets and compare its performance with several other approaches. All the approaches we compare with learn a Mahalanobis metric matrix and use coordinate descent method that ensures that each update of the metric matrix results inside the PD cone. 



\subsection{Competing approaches}



\textbf{COMET} - The algorithm described above.

\textbf{Euclidean} - The standard Euclidean distance in the feature space. The initialization of COMET using the identity matrix is equivalent to this distance measure.

\textbf{HDSL} - is an approach that deals with high-dimensional sparse data \cite{hdsl}. It is comprised on rank-1 sparse update matrices that are all zeros except for $2\times2$ pair of features elements. It greedily incorporates one pair of features at a time to control the number of active features.

\textbf{LEGO} - learns a Mahalanobis distance in an online fashion using a regularized per instance loss yielding a positive semidefinite matrix \cite{lego}. The main variant of LEGO aims to fit a given set of pairwise distances. We used another variant of LEGO that, like COMET, learns from relative distances. In our experimental setting, the loss is incurred for same-class samples being more than a certain distance away, and different class samples being less than a certain distance away.

\textbf{BoostMetric} - is based on the observation that any positive semidefinite matrix can be decomposed into linear positive combination of trace-1 rank-1 matrices  \cite{boost}. It uses rank-1 positive semidefinite matrices as weak learners within a boosting based learning process.

%OASIS??

\subsection{Datasets}
We evaluate COMET on two real world datasets of different sizes and dimensionality. 
\textbf{Reuters CV1} is a text classification dataset, with \textit{bag-of-words} representation. We take a 4 classes subset of Reuters CV1 introduced in \cite{CaiRCV14} and was tested for metric learning in \cite{hdsl}. We selected a subset of 5000 features that conveyed high information about the identity of the class using the \textit{infogain} criterion \cite{infogain}. This is a discriminative criterion, which measures the number of bits gained for category prediction by knowing the presence or absence of a term in a document. The selected features with normalized using \textit{tf-idf} and then represented each document as a bag of words.  
We used 100,000 triplets (and 200,000 LEGO constraints) for training Reuters CV1 which is the same scale that was used in \cite{hdsl}. For training HDSL, we took 8000 iterations as was taken by \cite{hdsl}

\textbf{Caltech256} is an image classification dataset.   We take a 50 classes subset of Caltech256 introduced and tested for metric learning in \cite{oasis}. It contains 65 images per class (total of 3250 images), represented with 975 \textit{bag-of-local-descriptors} features. We used 135,000 triplets (and 300,000 LEGO constraints) for training Caltech256 which is the same scale that was used in \cite{oasis}. We used early stopping after 8000 iterations to train HDSL. BoostMetric runs very slow and uses a hugh amount of memory. Therefore, we took the number of COMET coordinate steps to be the maximal number of BoostMetric rank1 updates
\ignore{
%\textbf{Semeion} is handwritten digits image dataset with binary sparse representation. That was downloaded from the UCI repository \cite{UCI, semeion} and was tested for metric learning in \cite{qian}. It contains 1593 images, each image is represented sparsely by 256 binary pixels features.

%\textbf{Protein} is a bioinformatics dataset that was downloaded from the LIBSVM repository \cite{libsvm} and was tested for metric learning  in \cite{qian}. It cons 24387 samples, each sample is represented by 357 features. 
}
\subsection{Experimental setup}
Two samples are considered similar if they share the same class label. Each data set is tested on a two layer 5 fold cross validation experiment with 80\%/20\% random splits. We use the same random splits across all approaches. We trained all the approaches with the exact same set of triplets, except for LEGO that uses pairs constraints and we've ensured that we choose triplets/constraints number in a regime that test performance converges. We generated the triplets randomly with a constant number of triplets per sample.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{COMET_convergence}}
\caption{ \textit{Precision-at-top-k} of COMET, evaluated on the test set, as a function of the number of coordinate steps, normalized to the data dimension. Error bars are standard error of the means across 5 random train/test partitions (80\%/20\%) }
\label{cometConvergeFig}
\end{center}
\vskip -0.2in
\end{figure} 

\subsection{Evaluation Measures}
We evaluated the performance of all algorithms using standard ranking precision measures
based on nearest neighbors. For each query image in the test set, all other test images were
ranked according to their similarity to the query image. The number of same-class images
among the top k images (the k nearest neighbors) was computed. When averaged across test
images (either within or across classes), this yields a measure known as \textit{precision-at-top-k},
providing a precision curve as a function of the rank $k$.

\subsection{Results}
 We evaluated the \textit{precision-at-top-k} on the test set, as a function of $k$ neighbours and averaged the results across 5 random train/test partitions (80\%/20\%).
Figure \ref{cometConvergeFig} traces the \textit{precision-at-top-k} over the test sets as it progresses during learning. We observe that convergence is usually achieved after (6 to 8)$\times d$ coordinate steps.
Figure \ref{precFig} compare the precision obtained with COMET, with four competing approaches, as described above. COMET achieved consistently superior or equal results throughout the full range of k (number of neighbours) tested. 


\todo{mention frob norm hasn't improved performance}
%  Note that we've also tested our approach with Frobenius norm regularization, but haven't seen a significant improvement for utilizing it.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Precision_at_K_all_datasets}}
\caption{ (best seen in color) Comparison of the performance of COMET, LEGO, BoostMetric, HDSL and the Euclidean metrics across the datasets . Each curve shows the \textit{precision-at-top-k}, evaluated on the test set, as a function of $k$ neighbours. The results are averaged across 5 train/test random partitions (80\%/20\%), error bars are standard error of the means (s.e.m.) }
\label{precFig}
\end{center}
\vskip -0.2in
\end{figure} 

\section{Summary}
We presented an approach for learning a distance metric from data samples,  continuously restricting the solutions to the positive cone, but avoiding runtime-costly projections. The approach is based on block-coordinate-descent




% ==============================================================
\appendix
\section*{Appendix A: Details of gradient derivation}

Next, we evaluate a matrix gradient step  $\frac{\partial {l_t (\W)}}{\partial \W}$ of an arbitrary triplet $t$. We define the linear part of the hinge loss of a triplet $t$ by
\begin{equation}
\lambda_t \eqdef \lambda_t(\W, \vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-}) \eqdef 1-\vec{q}_{t}\T \W \vec{p}_{t}^{+}+\vec{q}_{t}\T\W\vec{p}_{t}^{-}
\end{equation}

We note that $\W$ is PD and therefore symmetric, therefore we will require its gradient to be symmetric by replacing $\W$ with $\tfrac{1}{2}(\W + \W\T)$.

The derivative of the ranking loss is then given by
\begin{equation}
\frac{\partial {l_{\W}^{t}}}{\partial \W} = \tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T  + \Delta\vec{p}_{t}\vec{q}_{t}\T]\cdot {l'}(\lambda_t)
\label{dlossranking}
\end{equation} where $l'(x) \eqdef \frac{\partial {l(x)}}{\partial x}$, $\Delta\vec{p}_{t} \eqdef (\vec{p}_{t}^{-} - \vec{p}_{t}^{+})$

Replacing the linear hinge loss \eqref{hingelt} in \eqref{dlossranking}, we observe that it is not differentiable in $\lambda_t = 0$, but it has a sub gradient matrix

\begin{equation}
\frac{\partial {l_t (\W)}}{\partial \W} = \tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T  + \Delta\vec{p}_{t}\vec{q}_{t}\T] \cdot \textbf{1}(\lambda_t)
\end{equation}
where $\textbf{1}(x)$ is an indicator for $x>0$. Hence it is non-zero when \eqref{hingelt} is positive or zero otherwise. Note that replacing the linear hinge loss by a logarithmic loss would have yield a continues $\operatorname{sigmoid}(\lambda_t)$ instead of $\textbf{1}(\lambda_t)$.  We emphasis that 
$\frac{\partial {l_t (\W)}}{\partial \W}$ is a matrix, which represents the gradient of $l_{\W}^{t}$ with respect to each the elements of $W$.

The matrix gradient of $\tfrac{1}{2} \| \W \|_{F}^{2}$ equals $\W$ and the matrix gradient of  $\log \det(\W)$ equals $\W^{-1}$. Therefore, the matrix gradient of the objective function $L(\W)$ can be easily determined: 
\begin{multline}
\frac{\partial {L (\W)}}{\partial \W} = 
\sum\limits_{t\in T}{\{ [\tfrac{1}{2}[\vec{q}_{t}\Delta\vec{p}_{t}\T + \Delta\vec{p}_{t}\vec{q}_{t}\T \} } \cdot \textbf{1}(\lambda_t)] \\- \alpha \cdot \W^{-1} + \beta \cdot \W 
\label{gradMat}
\end{multline}


\section*{Appendix B: Updating the inverse matrices}
\label{appendix-inverse}

Nevertheless, evaluating \eqref{PDUpdateCondQuadForm} requires to
compute of $\W^{-1}$ and $\mat{A}^{-1}$. The most efficient
implementation is to update $\W^{-1}$ following a block coordinate
step, and to derive $\mat{A}^{-1}$ from $\W^{-1}$ before taking the
next step.


Next, we demonstrate how to evaluate $\W^{*-1}$ following a coordinate
step \eqref{updateEq} and how to evaluate $\mat{A}^{*-1}$ before
taking the next coordinate step. Both evaluations will cost a
computational complexity of $o(d^2)$.

Evaluating $\W^{*-1}$ can be easily taken using the Sbury matrix
identity \todo{addref}. By reformulating \eqref{updateEq} and
\eqref{gradMtx} according to the Woodbury matrix identity notation, we
can write
\begin{equation}
  \mat{W^*} = \W+\mat{\widetilde{G}}
  \label{updateEqWDB}
\end{equation}
where
\begin{equation}
  \mat{\widetilde{G}} = \mat{U}\mat{C}\mat{V} = \left[ \begin{matrix}
      \vec{u} & \vec{e_k} \end{matrix} \right] \left[ \begin{matrix}
      \eta & 0 \\ 0 & \eta \end{matrix} \right] \left[ \begin{matrix}
      \vec{e_k}\T \\ \vec{u}\T \end{matrix} \right]
  \label{gradMtxWDB}
\end{equation}

Utilizing the Woodbury matrix identity result with
\begin{equation}
\begin{array}{lcl}
\W^{*-1} = \\
\W^{-1} - \W^{-1} \mat{U} (\eta^{-1} I_{2x2} + \mat{V} \W^{-1} \mat{U})^{-1} \mat{V} \W^{-1}
\end{array}
\label{InvWwdb}
\end{equation}


Last, we evaluate $\mat{A}^{-1}$ before a coordinate step given $\W$
and $\W^{-1}$, using the Schur complement and its corresponding
notation \eqref{schurNotationPreUpdate}:
\begin{equation}
\begin{array}{l}
 \W^{-1} =  \\
 \left[ \begin{array}{cc} s & -s \vec{B}\T \mat{A}^{-1} \\ -(s \vec{B}\T \mat{A}^{-1} )\T & (\mat{A}^{-1} + \mat{A}^{-1} \vec{B} s \vec{B}\T \mat{A}^{-1} ) \end{array}  \right]
\end{array}
\label{BlockInvW}
\end{equation}
where $s$ is a scalar that denotes the Schur Complement:  $s \eqdef (\mat{C}-\vec{B}\T \mat{A}^{-1} \vec{B})$. Hence
\begin{equation}
  s = \W^{-1}_{(1,1)}
  \label{invWkk}
\end{equation}
\begin{equation*}
  -s \vec{B}\T \mat{A}^{-1} = -\W^{-1}_{(1,1)} \vec{B}\T \mat{A}^{-1} = \W^{-1}_{(1,2:d)}
\end{equation*}
\begin{equation*}
  \vec{B}\T \mat{A}^{-1} = -\frac{\W^{-1}_{(1,2:d)}}{\W^{-1}_{(1,1)} }
\end{equation*}
\begin{equation}
  \mat{A}^{-1}\vec{B} = (\vec{B}\T \mat{A}^{-1})\T
  \label{invA_Btrans}
\end{equation}
Assign above at the lower right block of \eqref{BlockInvW} results with
\begin{equation*}
  \mat{A}^{-1} + \frac{1}{\W^{-1}_{(1,1)} } \W^{-1}_{(2:d,1)} (\W^{-1}_{(2:d,1)})\T = \W^{-1}_{(2:d,2:d)}
\end{equation*}

Finally, $A^{-1}$ is extracted to be:
\begin{equation}
  \mat{A}^{-1} = \W^{-1}_{(2:d,2:d)}- \frac{\W^{-1}_{(2:,1)} \W^{-1}_{(1,2:d)}}{\W^{-1}_{(1,1)}}
  \label{InvA}
\end{equation}
and the computational complexity of \eqref{InvA} is $o(d^2)$



\bibliography{comet}
\bibliographystyle{icml2015}

\end{document} 

