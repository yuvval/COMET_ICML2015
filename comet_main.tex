%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% COMET ICML 2015 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

%% COMET packages
\usepackage{amssymb}
\usepackage{mathtools}

%%%%%%%%%%%%%%%%%%%%%%%%


% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Metric learning using Block Coordinate Descent}

\begin{document} 

\twocolumn[
\icmltitle{Metric Learning using Block Coordinate Descent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{metric learning, similarity learning}

\vskip 0.3in
]

%%%% COMET commands %%%%
\newcommand\todo[1]{\textbf{<ToDo:#1}>}
\newcommand\mat[1]{\mathcal{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\newcommand{\T}{{}^\mathsf{T}}
\newcommand{\W}{\mat{W}}
\newcommand{\newW}{{\mat{W^*}}}
\newcommand{\eqdef}{\doteq}
\newcommand{\Rn}{\mathbb{R}^n}

%\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 
TBD
\end{abstract} 

\section{Introduction}
Learning a measure of pair-wise similarity or distance among data samples is a fundamental task in machine learning. Learned metrics can be used to retrieve a similar image or document given an example \cite{}, or be used as a representation for a supervised learning technique that is based on distances, like nearest-neighbors or kernel methods \cite{}.


\section{Related work}
There has been a lot of work on learning similarity measures and distance metrics from data, see \cite{} for a review. Specifically, several approaches have been proposed for learning a positive definite Mahalanobis matrix. 

\section{Coordinate-descent metric learning}
We learn the metric using pairwise relations. One approach is to measure the similarity of two samples $\vec{q}, \vec{p} \in \Rn$ using a bilinear form parametrized by a model $\W \in \mathbb{R}^{n\times n}$.
\begin{equation}
S_{\W}(\vec{q}, \vec{p}) = \vec{q}\T \W \vec{p}
\end{equation}
To train our model, we generate a batch of triplets, such that three instances are sampled per triplet ($t$): a query $\vec{q}_t \in \Rn$ and two samples $\vec{p}_{t}^{+}, \vec{p}_{t}^{-} \in \Rn$. We wish that the model assigns a higher similarity score to the pair $(\vec{q}_t, \vec{p}_{t}^{+})$ than the pair $(\vec{q}_t, \vec{p}_{t}^{-})$, and hence use the ranking hinge loss per triplet $t$ which is defined as
\begin{equation}
l_{\W}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})=[1-\vec{q}_{t}\T \W \vec{p}_{t}^{+}+\vec{q}_{t}\T\W\vec{p}_{t}^{-}]_{+}
\label{hingelt}
\end{equation}
where $[z]_{+} \eqdef max(0,z)$. Such models have shown to achieve high precision \cite{oasis, qian}.
Then, we define an objective function of the total hinge loss over all the triplets and a $logdet$ regularization over $\W$ that assigns a high cost when $\W$ reaches close to the PD cone boundaries. 

We take a block coordinate descent approach which ensures that each step is taken within the PD cone. Each coordinate step is taken over a single row and column $k$ of $\W$. An upper bound on the step size is  evaluated using the Schur Complement condition for positive definiteness \todo{Add citation} and we take a step with a step size within that boundary. 

\subsection{The objective}
We define an objective function 
\begin{equation}
L(\W)=\sum\limits_{t\in T}{l_{\W}(\vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-})} - \mu \cdot logdet(\W) ) 
\end{equation}
where $T$ denotes the batch of triplets, $\mu$ is a hyper parameter and $logdet(\W) \eqdef log (\textbf{det}(\W).$ It is comprised of the total hinge loss over all the triplets and a $logdet$ regularization over $\W$ that assigns a finite high cost when $\W$ is PD and is close to the PD cone boundaries. 
Our objective is to find $\W$ that minimizes $L(\W)$, such that $\W$ is PD ($\W\succ 0$). Note that we've also tested our approach with Frobenius norm regularization, but haven't seen a significant improvement for utilizing it.

\subsection{Gradient evaluation}
Next, we evaluate a matrix gradient step  $\frac{\partial {l_t (\W)}}{\partial \W}$ of an arbitrary triplet $t$. We define the linear part of the hinge loss of a triplet $t$ by
\begin{equation}
\lambda_t \eqdef \lambda_t(\W, \vec{q}_t, \vec{p}_{t}^{+}, \vec{p}_{t}^{-}) \eqdef 1-\vec{q}_{t}\T \W \vec{p}_{t}^{+}+\vec{q}_{t}\T\W\vec{p}_{t}^{-}
%=max(0,1+trace(\W(vec{p}_{t}^{-} - vec{p}_{t}^{+})\vec{q}_{t}\T))
\end{equation}
Which also equals to
\begin{equation}
\lambda_t = 1+trace(\W(\vec{p}_{t}^{-} - \vec{p}_{t}^{+})\vec{q}_{t}\T))
\label{linloss}
\end{equation}

The hinge loss \eqref{hingelt} is not differentiable, but it has a sub gradient matrix. We can evaluate it by applying \eqref{linloss} in \eqref{hingelt}.
\begin{equation}
\frac{\partial {l_t (\W)}}{\partial \W} = \vec{q}_{t}(\vec{p}_{t}^{-} - \vec{p}_{t}^{+}) \cdot \textbf{1}(\lambda_t)
\end{equation}
where $\textbf{1}(x)$ is an indicator for $x>0$. Hence it is non-zero when \eqref{hingelt} is positive or zero otherwise. We emphasis that 
$\frac{\partial {l_t (\W)}}{\partial \W}$ is a matrix, which represents the gradient of $l_t(\W)$ with respect to each the elements of $W$.

The matrix gradient of $logdet(\W)$ equals $\W^{-1}$. Therefore, the matrix gradient of the objective function $L(\W)$ can be easily determined: 
\begin{equation}
\frac{\partial {L (\W)}}{\partial \W} = \sum\limits_{t\in T}{[\vec{q}_{t}(\vec{p}_{t}^{-} - \vec{p}_{t}^{+}) \cdot \textbf{1}(\lambda_t)]} - \mu \cdot \W^{-1}
\end{equation}

Taking the gradient of the objective which is the sum of of hinge losses over all the triplets in a batch is a straight-forward gradient descent approach and has already been utilized by \cite{qian}. Its drawback is that it can't ensure that the update following the gradient step will keep $\W$ as positive definite. On \cite{qian} a projection to the PSD cone is applied following each update. 

Next, we introduce our novelty for taking a (block coordinate) gradient step that ensures that if $\W$ before the update is PD, then $\W$ following the update will be kept as PD.

\subsection{Block coordinate-descent steps}




\section{Convergence}
URI?

\section{Experiments}
\subsection{Experimental setup}
\subsection{Competing methods}
\subsection{Retrieval of similar images}
\subsection{Document retrieval}


\section{Summary}
This is the best paper ever.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{comet}
\bibliographystyle{icml2015}

\end{document} 

